<<<<<<< Updated upstream
library(OpenML)
library(DALEX)
library(OpenML)
install.packages("OpenML")
library(OpenML)
library("OpenML", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.4")
detach("package:OpenML", unload=TRUE)
remove.packages("OpenML", lib="~/R/x86_64-pc-linux-gnu-library/3.4")
install.packages("OpenML")
remove.packages("curl", lib="~/R/x86_64-pc-linux-gnu-library/3.4")
install.packages("curl")
install.packages("curl")
install.packages("OpenML")
library(readr)
data_train <- read_csv("Desktop/data_train.csv")
View(data_train)
data<-data_train[1:100,]
library(dplyr)
View(data)
(3750901.5068+3770901.5068)/2
(-19268905.6133-19208905.6133)/2
data %>% mutate(is_closer=dist(rbind(c(x_entry,y_entry),c(3760902,-19238906))>dist(rbind(c(x_exit,y_exit),c(3760902,-19238906))))
)
data %>% mutate(is_closer=dist(rbind(c(x_entry,y_entry),c(3760902,-19238906))>dist(rbind(c(x_exit,y_exit),c(3760902,-19238906)))))
data %>% mutate(is_closer=dist(rbind(c(x_entry,y_entry),c(3760902,-19238906))>dist(rbind(c(x_exit,y_exit),c(3760902,-19238906))))) ->tmp
View(tmp)
data$y_exit<- -19238906
data$x_exit<- 3760902
data %>% mutate(is_closer=dist(rbind(c(x_entry,y_entry),c(3760902,-19238906))>dist(rbind(c(x_exit,y_exit),c(3760902,-19238906))))) ->tmp
View(tmp)
data %>% mutate(is_closer=dist(rbind(c(x_entry,y_entry),c(3760902,-19238906)))>dist(rbind(c(x_exit,y_exit),c(3760902,-19238906)))) ->tmp
View(tmp)
data<-data_train[1:100,]
data %>% mutate(is_closer=dist(rbind(c(x_entry,y_entry),c(3760902,-19238906)))>dist(rbind(c(x_exit,y_exit),c(3760902,-19238906)))) ->tmp
View(tmp)
data$y_exit<- -19238906
data$x_exit<- 3760902
data %>% mutate(is_closer=dist(rbind(c(x_entry,y_entry),c(3760902,-19238906)))>dist(rbind(c(x_exit,y_exit),c(3760902,-19238906)))) ->tmp
View(tmp)
?dist()
data %>% mutate(is_closer=dist(rbind(c(x_entry,y_entry),c(3760902,-19238906)))) ->tmp
View(tmp)
data<-data_train[1:100,]
data %>% mutate(is_closer=dist(rbind(c(x_entry,y_entry),c(3760902,-19238906)))) ->tmp
View(tmp)
View(data)
mean(is.numeric(data_train$vmean))
is.numeric(data_train$vmean)
data_train$vmean
is.na(data_train$vmean)
mean(is.na(data_train$vmean) | data$vmean=="NaN")
data$vmean=="NaN"
mean(data$vmean=="NaN")
mean(data$vmean=="NaN",na.rm = TRUE)
mean(is.na(data_train$vmean))
mean(is.na(data_train$vmean))*mean(data$vmean=="NaN",na.rm = TRUE)
mean(data$vmean=="NaN",na.rm = TRUE)/mean(is.na(data_train$vmean))
#>dist(rbind(c(x_exit,y_exit),c(3760902,-19238906)))
mean(data$vmean=="NaN",na.rm = TRUE)
mean(is.na(data_train$vmean))
(1-mean(is.na(data_train$vmean)))*mean(data$vmean=="NaN",na.rm = TRUE)
(1-mean(is.na(data_train$vmax)))*mean(data$vmax=="NaN",na.rm = TRUE)
(1-mean(is.na(data_train$vmin)))*mean(data$vmin=="NaN",na.rm = TRUE)
setwd("~/Desktop/2019L-WUM/Zadania_domowe/Zadanie_domowe_5/Olaf_Werner")
library(mlr)
set.seed(123, "L'Ecuyer")
library(mlr)
#sprawdzamy paczki
listLearners(check.packages = TRUE)
#robimy taska i learnera
classif_task = makeClassifTask(id = "task", data = train, target =dane$target.features)
dane<-Titanic
dane
Titanic
install.packages("titanic")
library(titanic)
titanic::titanic_train
library(readr)
train <- read_csv("titanic/kaggle-titanic-master/input/train.csv")
View(train)
#robimy taska i learnera
classif_task <- makeClassifTask(id = "task", data = train, target =)
#robimy taska i learnera
classif_task <- makeClassifTask(id = "task", data = train, target = "Survived")
-1:-2
train<-train[-1:-2]
length(unique(train$Ticket))
length(unique(train$Embarked))
length(unique(train$SibSp))
library(DataExplorer)
train <- read_csv("titanic/kaggle-titanic-master/input/train.csv")
train<-drop_columns(train,c("Name","PassengerId"))
View(train)
library(dplyr)
train[sapply(train, is.character)] <- lapply(train[sapply(train, is.character)],as.factor)
train[!is.na(train$Cabin),]
unique(train[!is.na(train$Cabin),]$Cabin)
length(unique(train[!is.na(train$Cabin),]$Cabin))
train$Survived<-as.factor(train$Survived)
View(train)
train$Survived
set.seed(123, "L'Ecuyer")
library(mlr)
library(readr)
library(DataExplorer)
library(dplyr)
train <- read_csv("titanic/kaggle-titanic-master/input/train.csv")
train<-drop_columns(train,c("Name","PassengerId"))
train[sapply(train, is.character)] <- lapply(train[sapply(train, is.character)],as.factor)
train$Survived<-as.factor(train$Survived)
#robimy taska i learnera
classif_task <- makeClassifTask(id = "task", data = train, target = "Survived")
classif_learner<-makeLearner("classif.rpart",predict.type = "prob")
#testy Acc, AUC, Specificity, Recall, Precision, F1
Rcuda<-list(f1=f1, acc=acc, auc=auc, tnr=tnr, tpr=tpr ,ppv=ppv)
measures<-intersect(listMeasures(classif_task),c("f1", "acc", "auc", "tnr", "tpr" ,"ppv"))
#testy Acc, AUC, Specificity, Recall, Precision, F1
cv <- makeResampleDesc("CV", iters = 5)
r <- resample(classif_learner, classif_task, cv,measures = list(acc,auc,tnr,tpr,ppv,f1))
r<-r$aggr
r
?rpart::plotcp()
?resample
install.packages("rpart.plot")
install.packages("rpart")
train(learner,task)
mlr::train(learner,task)
set.seed(123, "L'Ecuyer")
library(mlr)
library(readr)
library(DataExplorer)
library(dplyr)
train <- read_csv("titanic/kaggle-titanic-master/input/train.csv")
train<-drop_columns(train,c("Name","PassengerId"))
train[sapply(train, is.character)] <- lapply(train[sapply(train, is.character)],as.factor)
train$Survived<-as.factor(train$Survived)
#robimy taska i learnera
task <- makeClassifTask(id = "task", data = train, target = "Survived")
learner<-makeLearner("classif.rpart",predict.type = "prob")
mlr::train(learner,task)
tree<-mlr::train(learner,task)
View(tree)
plot(tree)
library(rpart)
?rpart
print(tree)
plot(tree)
plot(tree$learner)
plot(tree$learner.model)
library(rpart.plot)
plot(tree$learner.model)
plot(tree)
rpart.plot(tree)
rpart.plot(tree$learner)
rpart.plot(tree$learner.model)
train <- read_csv("titanic/kaggle-titanic-master/input/train.csv")
train<-drop_columns(train,c("Name","PassengerId"))
train[sapply(train, is.character)] <- lapply(train[sapply(train, is.character)],as.factor)
train$Survived<-as.factor(train$Survived)
train$Cabin<-as.factor(!is.na(train$Cabin))
as.logical(21)
train <- read_csv("titanic/kaggle-titanic-master/input/train.csv")
train<-drop_columns(train,c("Name","PassengerId"))
train[sapply(train, is.character)] <- lapply(train[sapply(train, is.character)],as.factor)
train$Survived<-as.factor(as.logical(train$Survived))
train$Cabin<-as.factor(!is.na(train$Cabin))
View(train)
unique(train$Ticket)
train <- read_csv("titanic/kaggle-titanic-master/input/train.csv")
train<-drop_columns(train,c("Name","PassengerId","Ticket"))
train[sapply(train, is.character)] <- lapply(train[sapply(train, is.character)],as.factor)
train$Survived<-as.factor(as.logical(train$Survived))
train$Cabin<-as.factor(!is.na(train$Cabin))
#robimy taska i learnera
task <- makeClassifTask(id = "task", data = train, target = "Survived")
learner<-makeLearner("classif.rpart",predict.type = "prob")
tree<-mlr::train(learner,task)
#testy Acc, AUC, Specificity, Recall, Precision, F1
cv <- makeResampleDesc("CV", iters = 5)
r <- resample(learner, task, cv,measures = list(acc,auc,tnr,tpr,ppv,f1))
r<-r$aggr
r
rpart.plot(tree$learner.model)
table(train$Survived)
library(DataExplorer)
library(dplyr)
set.seed(123, "L'Ecuyer")
library(mlr)
library(readr)
library(DataExplorer)
library(dplyr)
library(rpart)
library(rpart.plot)
train <- read_csv("titanic/kaggle-titanic-master/input/train.csv")
train<-drop_columns(train,c("Name","PassengerId","Ticket"))
train[sapply(train, is.character)] <- lapply(train[sapply(train, is.character)],as.factor)
train$Survived<-as.factor(as.logical(train$Survived))
train$Cabin<-as.factor(!is.na(train$Cabin))
#robimy taska i learnera
task <- makeClassifTask(id = "task", data = train, target = "Survived")
learner<-makeLearner("classif.rpart",predict.type = "prob")
setwd("~/Desktop/WUM_projekt1")
library(tidyverse)
library(dplyr)
library(FNN)
library(gbm)
library(mlr)
library(randomForest)
library(e1071)
library(ranger)
library(MASS)
library(DALEX)
library(xgboost)
library(DataExplorer)
heloc_ok<-read_csv("heloc_ok.csv")
View(heloc_ok)
heloc_ok<-heloc_ok[-1]
give_me_AUC<-function(train_set)
{
n<-ncol(train_set)-1;
cv <- makeResampleDesc("CV", iters = 5)
#Random Froest
task <- makeClassifTask(data = train_set, target = "RiskPerformance")
model_all_rf <- makeLearner("classif.randomForest",
predict.type = "prob",
ntree = 100)
auc_all_rf<-r <- resample(model_all_rf, task, cv,measures=list(acc))
AUC<-auc_all_rf$aggr
name<-"rf"
number_of_cols<-n
AUC
# SVM
model_all_svm <- makeLearner("classif.svm",
predict.type = "prob")
auc_all_svm<-resample(model_all_svm, task, cv,measures=list(acc))
AUC<-append(AUC, auc_all_svm$aggr)
name<-append(name, "svm")
number_of_cols<-append(n, n)
#rpart
model_all_rpart <- makeLearner("classif.rpart",
predict.type = "prob")
auc_all_rpart<-resample(model_all_rpart, task, cv,measures=list(acc))
AUC<-append(AUC, auc_all_rpart$aggr)
name<-append(name, "rpart")
number_of_cols<-append(n, n)
#qda
model_all_qda <- makeLearner("classif.qda",
predict.type = "prob")
auc_all_qda<-resample(model_all_qda, task, cv,measures=list(acc))
AUC<-append(AUC, auc_all_qda$aggr)
name<-append(name, "qda")
number_of_cols<-append(n, n)
#lda
model_all_lda <- makeLearner("classif.lda",
predict.type = "prob")
auc_all_lda<-resample(model_all_lda, task, cv,measures=list(acc))
AUC<-append(AUC, auc_all_lda$aggr)
name<-append(name, "lda")
number_of_cols<-append(n, n)
#naive Bayes
model_all_nb <- makeLearner("classif.naiveBayes",
predict.type = "prob")
auc_all_nb<-resample(model_all_nb, task, cv,measures=list(acc))
AUC<-append(AUC, auc_all_nb$aggr)
name<-append(name, "nb")
number_of_cols<-append(n, n)
A<-data.frame(AUC=AUC, Classifier=name, number_of_cols=number_of_cols)
}
score1<-give_me_AUC(heloc_ok)
View(score1)
setwd("~/Desktop/2019L-WUM/Zadania_domowe/Zadanie_domowe_5/Olaf_Werner")
set.seed(123, "L'Ecuyer")
library(mlr)
library(readr)
library(DataExplorer)
library(dplyr)
library(rpart)
library(rpart.plot)
train <- read_csv("titanic/kaggle-titanic-master/input/train.csv")
train<-drop_columns(train,c("Name","PassengerId","Ticket"))
train[sapply(train, is.character)] <- lapply(train[sapply(train, is.character)],as.factor)
train$Survived<-as.factor(as.logical(train$Survived))
train$Cabin<-as.factor(!is.na(train$Cabin))
#robimy taska i learnera
task <- makeClassifTask(id = "task", data = train, target = "Survived")
learner<-makeLearner("classif.rpart",predict.type = "prob")
tree<-mlr::train(learner,task)
#testy Acc, AUC, Specificity, Recall, Precision, F1
cv <- makeResampleDesc("CV", iters = 5)
r <- resample(learner, task, cv,measures = list(acc,auc,tnr,tpr,ppv,f1))
r$models
r$extract
r <- resample(learner, task, cv,extract = TRUE,measures = list(acc,auc,tnr,tpr,ppv,f1))
set.seed(123, "L'Ecuyer")
library(mlr)
library(readr)
library(DataExplorer)
library(dplyr)
library(rpart)
library(rpart.plot)
train <- read_csv("titanic/kaggle-titanic-master/input/train.csv")
train<-drop_columns(train,c("Name","PassengerId","Ticket"))
train[sapply(train, is.character)] <- lapply(train[sapply(train, is.character)],as.factor)
train$Survived<-as.factor(as.logical(train$Survived))
train$Cabin<-as.factor(!is.na(train$Cabin))
#robimy taska i learnera
task <- makeClassifTask(id = "task", data = train, target = "Survived")
learner_default<-makeLearner("classif.rpart",predict.type = "prob")
learner_default<-makeLearner("classif.rpart",predict.type = "prob")
getHyperPars(learner = learner_default)
getParamSet(learner_default)
#getParamSet(learner_default)
learner_article<-makeLearner("classif.rpart",predict.type = "prob",par.vals = list(cp=0.001,maxdepth=13,minbucket=12,minsplit=18))
#testy Acc, AUC, Specificity, Recall, Precision, F1
cv <- makeResampleDesc("CV", iters = 5)
r <- resample(learner_article, task, cv,measures = list(acc,auc,tnr,tpr,ppv,f1))
r<-r$aggr
r
#testy Acc, AUC, Specificity, Recall, Precision, F1
cv <- makeResampleDesc("CV", iters = 5)
r_article <- resample(learner_article, task, cv,measures = list(acc,auc,tnr,tpr,ppv,f1))
r_article<-r_article$aggr
r_article
r_default <- resample(learner_default, task, cv,measures = list(acc,auc,tnr,tpr,ppv,f1))
r_default<-r_default$aggr
r_default
tree<-mlr::train(learner_article,task)
rpart.plot(tree$learner.model)
print(tree)
?makeNumericParam
getParamSet(learner_default)
rpart_pars <- tuneParams(
makeLearner("classif.rpart",predict.type = "prob"),
subsetTask(makeClassifTask(id = "task", data = train, target = "Survived")),
resampling = cv5,
measures = mlr::auc,
par.set = makeParamSet(
makeNumericParam("cp",lower = 0,upper = 1),
makeDiscreteParam("maxdepth", values = 1:30),
makeDiscreteParam("minbucket", values = 1:40),
makeDiscreteParam("minsplit", values = 1:40)
),
control = makeTuneControlRandom(maxit = 200)
)
rpart_pars$learner
rpart_pars$control
rpart_pars$x
learner_random<-makeLearner("classif.rpart",predict.type = "prob",par.vals = rpart_pars$x)
?resample
#testy Acc, AUC, Specificity, Recall, Precision, F1
cv <- makeResampleDesc("CV", iters = 5)
r_article <- resample(learner_article, task, cv,measures = list(acc,auc,tnr,tpr,ppv,f1))
View(r_article)
#testy Acc, AUC, Specificity, Recall, Precision, F1
cv <- makeResampleDesc("CV", iters = 5)
r_article <- resample(learner_article, task, cv,measures = list(acc,auc,tnr,tpr,ppv,f1),extract = function(x){getLearnerModel(x)})
r_article$extract
rpart.plot(r_article$extract)
rpart.plot(r_article$extract[1])
rpart.plot(r_article$extract[[1]])
r_article$measures.test
r_article$measures.train
r_article$measures.test
r_article$measures.test$auc
which.max(r_article$measures.test$auc)
#testy Acc, AUC, Specificity, Recall, Precision, F1
cv <- makeResampleDesc("CV", iters = 5)
r_article <- resample(learner_article, task, cv,measures = list(acc,auc,tnr,tpr,ppv,f1),extract = function(x){getLearnerModel(x)},show.info = FALSE)
rpart.plot(r_article$extract[[which.max(r_article$measures.test$auc)]])
r_article$aggr
?tuneParams
library(knitr)
knitr::kable(rbind(r_article$aggr,r_default$aggr,r_random$aggr),row.names = c("article","default","random"))
r_article <- resample(learner_article, task, cv,measures = list(auc),extract = function(x){getLearnerModel(x)},show.info = FALSE)
#rpart.plot(r_article$extract[[which.max(r_article$measures.test$auc)]])
r_article$aggr
r_default <- resample(learner_default, task, cv,measures = list(auc),extract = function(x){getLearnerModel(x)},show.info = FALSE)
#rpart.plot(r_default$extract[[which.max(r_default$measures.test$auc)]])
r_default$aggr
r_random <- resample(learner_random, task, cv,measures = list(auc),extract = function(x){getLearnerModel(x)},show.info = FALSE)
#rpart.plot(r_random$extract[[which.max(r_random$measures.test$auc)]])
r_random$aggr
knitr::kable(rbind(r_article$aggr,r_default$aggr,r_random$aggr),row.names = c("article","default","random"))
rbind(r_article$aggr,r_default$aggr,r_random$aggr)
knitr::kable(rbind(r_article$aggr,r_default$aggr,r_random$aggr))
?kable()
podsumowanie<-rbind(r_article$aggr,r_default$aggr,r_random$aggr)
rownames(podsumowanie)<-c("article","default","random")
knitr::kable(podsumowanie,row.names = TRUE)
getParamSet(learner_default)
learner_information<-makeLearner("classif.rpart",predict.type = "prob",par.vals = list(parms = list(split = 'information')))
?rpart.plot
par(mfrow=c(1,2))
learner_information<-makeLearner("classif.rpart",predict.type = "prob",par.vals = list(parms = list(split = 'information')))
tree_default<-mlr::train(learner_default,task)
rpart.plot(tree_default$learner.model)
tree_information<-mlr::train(learner_information,task)
rpart.plot(tree_information$learner.model)
setwd("~/Desktop/chtg")
source("functions.R")
install.packages("igraph")
install.packages("igraph")
install.packages("igraph")
install.packages("igraph")
install.packages("igraph")
install.packages("igraph")
### example calls ----
graf <- make_weighted_line_graph(file_to_igraph("myciel3.col"))
source("functions.R")
### example calls ----
graf <- make_weighted_line_graph(file_to_igraph("myciel3.col"))
View(graf)
plot(induced_subgraph(graf, get_heaviest_vertice_neighborhood(graf)))
plot(get_subgraph_without_neighborhood(graf, 1))
ggraf2 <- get_subgraph_without_neighborhood(graf2, get_heaviest_vertex(graf2))
plot(ggraf2, vertex.label = V(ggraf2)$names, vertex.color = V(ggraf2)$weights)
igraph_demo()
igraph_demo(smallworld())
smallworld()
?smallworld
g <- sample_smallworld(1, 100, 5, 0.05)
plot(g)
### example calls ----
graf <- make_weighted_line_graph(file_to_igraph("myciel3.col"))
plot(induced_subgraph(graf, get_heaviest_vertice_neighborhood(graf)))
get_heaviest_vertice_neighborhood(graf)
plot(induced_subgraph(graf, get_heaviest_vertex_neighborhood(graf)))
plot(get_subgraph_without_neighborhood(graf, 1))
ggraf2 <- get_subgraph_without_neighborhood(graf2, get_heaviest_vertex(graf2))
ggraf2 <- get_subgraph_without_neighborhood(graf, get_heaviest_vertex(graf))
plot(ggraf2, vertex.label = V(ggraf2)$names, vertex.color = V(ggraf2)$weights)
setwd("~/Desktop/WUM_projekt1")
knitr::opts_chunk$set(echo = TRUE)
knitr::include_graphics("krzywa_git.png")
knitr::include_graphics("boxploty.png")
# Libraries
knitr::opts_chunk$set(echo = FALSE)
library(readr)
library(mlr)
library(ggplot2)
library(DALEX)
library(knitr)
# Wczytywanie datasetu
# readr::read_csv("final_dataset.csv", col_types = cols(
#   library = col_factor(),
#   model_name = col_factor(),
#   numberOfCategoricalFeatures = col_double(),
#   numberOfNumericalFeatures = col_double(),
#   meanUniqueNumericalValues = col_double(),
#   meanUniqueCategoricalValues = col_double(),
#   meanNumberMissing = col_double(),
#   number_of_instances = col_double(),
#   ACC = col_double()
# )) -> df
# Dataset preparation
# df <- df[!is.na(df$meanUniqueNumericalValues), ]
# df <- df[!is.na(df$meanUniqueCategoricalValues), ]
knitr::include_graphics("ROC.png")
knitr::opts_chunk$set(echo = FALSE,warning = FALSE)
library(readr)
library(mlr)
library(ggplot2)
library(DALEX)
library(knitr)
heloc_ok<-read_csv("heloc_ok.csv")
heloc_ok<-heloc_ok[-1]
heloc_ok$MaxDelqEver<-factor(heloc_ok$MaxDelqEver)
heloc_ok$MaxDelq2PublicRecLast12M<-factor(heloc_ok$MaxDelq2PublicRecLast12M)
heloc_ok<-mlr::createDummyFeatures(heloc_ok)
heloc_ok$RiskPerformance<-factor(heloc_ok$RiskPerformance)
heloc_dataset_v1 <- read_csv("heloc_dataset_v1.csv")
heloc_dataset_v1$RiskPerformance<-factor(heloc_dataset_v1$RiskPerformance)
task<- makeClassifTask(id = "task", data = heloc_dataset_v1, target ="RiskPerformance")
learner_rpart<-makeLearner("classif.randomForest",predict.type = "prob")
cv <- makeResampleDesc("CV", iters = 5)
test_rpart <- resample(learner_rpart, task, cv,measures = auc,show.info = FALSE)
print(test_rpart)
roc_r = generateThreshVsPerfData(test_rpart$pred, list(fpr, tpr), aggregate = TRUE)
roc_r = generateThreshVsPerfData(test_rpart$pred, list(fpr, tpr), aggregate = TRUE)
plotROCCurves(roc_r)
?plotROCCurves
roc_r
=======
ACC_org<-sum(diag(table(wina$Klasa, predict(tree, wina))))/sum(table(wina$Klasa, predict(tree, wina)))
task<-mlr::makeClassifTask(data=wina, target = "Klasa")
learner<-makeLearner("classif.rpart", predict.type = "response", par.vals = list( minsplit = 1, minbucket = 1, cp = 0))
cv <- makeResampleDesc("CV", iters = 5)
r <- resample(learner, task, cv, measures=list(acc))
ACC_cros<- r$aggr
kable( data.frame(kroswalidacja=ACC_cros, podstawinie=ACC_org))
tree_table<-data.frame(printcp(tree))
min_xerror<-min(tree_table$xerror)
index<-which.min(tree_table$xerror)
xstd<-tree_table[which.min(tree_table$xerror),5]
xerrors<-which.max(tree_table[1:(index-1),4] > min_xerror & tree_table[1:(index-1),4] < (xstd+min_xerror))
xerrors
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(rpart)
library(rpart.plot)
library(mlr)
library (knitr)
library(rmarkdown)
set.seed(1000)
rm(list=ls())
wina<-read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data", sep =",");
colnames(wina)<-c("Klasa", "Alkohol", "Kwas_jablkowy", "Popiol", "Alkaicznosc_popiolu", "Magnez","Calkowite_fenole", "Flawonoidy", "Nonflawanoidowe_fenole", "Proantocyjany", "Intensywnosc_koloru", "Odcien",  "Rozcienczenie_win", "Prolina" )
tree <- rpart(Klasa ~ ., wina, minsplit = 1, minbucket = 1, cp = 0)
rpart.plot(tree, type = 1, extra = 1)
ACC_org<-sum(diag(table(wina$Klasa, predict(tree, wina))))/sum(table(wina$Klasa, predict(tree, wina)))
task<-mlr::makeClassifTask(data=wina, target = "Klasa")
learner<-makeLearner("classif.rpart", predict.type = "response", par.vals = list( minsplit = 1, minbucket = 1, cp = 0))
cv <- makeResampleDesc("CV", iters = 5)
r <- resample(learner, task, cv, measures=list(acc))
ACC_cros<- r$aggr
kable( data.frame(kroswalidacja=ACC_cros, podstawinie=ACC_org))
tree_table<-data.frame(printcp(tree))
min_xerror<-min(tree_table$xerror)
index<-which.min(tree_table$xerror)
xstd<-tree_table[which.min(tree_table$xerror),5]
xerrors<-which.max(tree_table[1:(index-1),4] > min_xerror & tree_table[1:(index-1),4] < (xstd+min_xerror))
xerrors
set.seed(888)
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(rpart)
library(rpart.plot)
library(mlr)
library (knitr)
library(rmarkdown)
set.seed(888)
rm(list=ls())
wina<-read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data", sep =",");
colnames(wina)<-c("Klasa", "Alkohol", "Kwas_jablkowy", "Popiol", "Alkaicznosc_popiolu", "Magnez","Calkowite_fenole", "Flawonoidy", "Nonflawanoidowe_fenole", "Proantocyjany", "Intensywnosc_koloru", "Odcien",  "Rozcienczenie_win", "Prolina" )
tree <- rpart(Klasa ~ ., wina, minsplit = 1, minbucket = 1, cp = 0)
rpart.plot(tree, type = 1, extra = 1)
ACC_org<-sum(diag(table(wina$Klasa, predict(tree, wina))))/sum(table(wina$Klasa, predict(tree, wina)))
task<-mlr::makeClassifTask(data=wina, target = "Klasa")
learner<-makeLearner("classif.rpart", predict.type = "response", par.vals = list( minsplit = 1, minbucket = 1, cp = 0))
cv <- makeResampleDesc("CV", iters = 5)
r <- resample(learner, task, cv, measures=list(acc))
ACC_cros<- r$aggr
kable( data.frame(kroswalidacja=ACC_cros, podstawinie=ACC_org))
tree_table<-data.frame(printcp(tree))
min_xerror<-min(tree_table$xerror)
index<-which.min(tree_table$xerror)
xstd<-tree_table[which.min(tree_table$xerror),5]
xerrors<-which.max(tree_table[1:(index-1),4] > min_xerror & tree_table[1:(index-1),4] < (xstd+min_xerror))
xerrors
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(rpart)
library(rpart.plot)
library(mlr)
library (knitr)
library(rmarkdown)
set.seed(198)
rm(list=ls())
wina<-read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data", sep =",");
colnames(wina)<-c("Klasa", "Alkohol", "Kwas_jablkowy", "Popiol", "Alkaicznosc_popiolu", "Magnez","Calkowite_fenole", "Flawonoidy", "Nonflawanoidowe_fenole", "Proantocyjany", "Intensywnosc_koloru", "Odcien",  "Rozcienczenie_win", "Prolina" )
tree <- rpart(Klasa ~ ., wina, minsplit = 1, minbucket = 1, cp = 0)
rpart.plot(tree, type = 1, extra = 1)
ACC_org<-sum(diag(table(wina$Klasa, predict(tree, wina))))/sum(table(wina$Klasa, predict(tree, wina)))
task<-mlr::makeClassifTask(data=wina, target = "Klasa")
learner<-makeLearner("classif.rpart", predict.type = "response", par.vals = list( minsplit = 1, minbucket = 1, cp = 0))
cv <- makeResampleDesc("CV", iters = 5)
r <- resample(learner, task, cv, measures=list(acc))
ACC_cros<- r$aggr
kable( data.frame(kroswalidacja=ACC_cros, podstawinie=ACC_org))
tree_table<-data.frame(printcp(tree))
min_xerror<-min(tree_table$xerror)
index<-which.min(tree_table$xerror)
xstd<-tree_table[which.min(tree_table$xerror),5]
xerrors<-which.max(tree_table[1:(index-1),4] > min_xerror & tree_table[1:(index-1),4] < (xstd+min_xerror))
xerrors
best_param<-which.max(tree_table[1:(index-1),4] > min_xerror & tree_table[1:(index-1),4] < (xstd+min_xerror))
tree_param <- rpart(Klasa ~ ., wina, minsplit = 1 , minbucket = 1, cp = tree_table[best_param,1])
rpart.plot(tree_param, type = 1, extra = 1)
kable( data.frame(kroswalidacja=ACC_cros, podstawinie=ACC_org))
ACC_param<-ACC_org_coss(wina, tree_param)
ACC_org_coss<-function(wina, tree){
ACC_org<-sum(diag(table(wina$Klasa, predict(tree, wina))))/sum(table(wina$Klasa, predict(tree, wina)))
task<-mlr::makeClassifTask(data=wina, target = "Klasa")
learner<-makeLearner("classif.rpart", predict.type = "response", par.vals = list( minsplit = 1, minbucket = 1, cp = 0))
cv <- makeResampleDesc("CV", iters = 5)
r <- resample(learner, task, cv, measures=list(acc))
ACC_cros<- r$aggr
kable( data.frame(kroswalidacja=ACC_cros, podstawinie=ACC_org))
}
ACC_param<-ACC_org_coss(wina, tree_param)
ACC_param
ACC<-ACC_org_coss(wina, tree)
ACC
rownames(ACC_param, c("ACC"))
rownames(ACC_param, "AA")
trees<-lapply(2:14, function(i)rpart(Klasa~., wina[1:i], minsplit = 0, minbucket = 0, cp = 0 ))
View(trees)
View(trees)
trees_param<-prune(trees, cp=choose.cp(trees))
lapply(1:13, function(i)prune(trees[i], cp=choose.cp(trees[i])))
trees[1]
trees
trees<-lapply(2:14, function(i)rpart(Klasa~., wina[1:i], minsplit = 1, minbucket = 1, cp = 0 ))
trees
lapply(1:13, function(i)prune(trees[i], cp=choose.cp(trees[i])))
trees<-lapply(2:14, function(i) rpart(Klasa~., wina[1:i], minsplit = 1, minbucket = 1, cp = 0 ))
prune(trees[1], cp=choose.cp(trees[1]))
?pune
?prune
??prune
choose.cp <- function(tree) {
n <- which.min(tree$cptable[, 4])
n.min <- min(which(tree$cptable[, 4] < tree$cptable[n, 4] + tree$cptable[n, 5]))
return(tree$cptable[n.min, 1])
}
trees<-lapply(2:14, function(i) rpart(Klasa~., wina[1:i], minsplit = 1, minbucket = 1, cp = 0 ))
lapply(1:13, function(i)prune(trees[i], cp=choose.cp(trees[i])))
class(tree)
class(trees[1])
tree
trees[1]
trees[2]
tree
trees[1]
trees[4]
repart(trees[4])
rpart(trees[4])
trees<-lapply(2:14, function(i) rpart(Klasa~., wina[1:i], minsplit = 1, minbucket = 1, cp = 0 ))
trees_param<- lapply(2:14, function(i)rpart(rpart(Klasa~., wina[1:i], minsplit = 1, minbucket = 1, cp=choose.cp(trees[i-1]))))
trees<-lapply(2:14, function(i) rpart(Klasa~., wina[,1:i], minsplit = 1, minbucket = 1, cp = 0 ))
trees_param<- lapply(2:14, function(i) rpart(rpart(Klasa~., wina[,1:i], minsplit = 1, minbucket = 1, cp=choose.cp(trees[i-1]))))
View(tree_param)
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(rpart)
library(rpart.plot)
library(mlr)
library (knitr)
library(rmarkdown)
set.seed(198)
rm(list=ls())
wina<-read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data", sep =",");
colnames(wina)<-c("Klasa", "Alkohol", "Kwas_jablkowy", "Popiol", "Alkaicznosc_popiolu", "Magnez","Calkowite_fenole", "Flawonoidy", "Nonflawanoidowe_fenole", "Proantocyjany", "Intensywnosc_koloru", "Odcien",  "Rozcienczenie_win", "Prolina" )
tree <- rpart(Klasa ~ ., wina, minsplit = 1, minbucket = 1, cp = 0)
rpart.plot(tree, type = 1, extra = 1)
ACC_org_coss<-function(wina, tree){
ACC_org<-sum(diag(table(wina$Klasa, predict(tree, wina))))/sum(table(wina$Klasa, predict(tree, wina)))
task<-mlr::makeClassifTask(data=wina, target = "Klasa")
learner<-makeLearner("classif.rpart", predict.type = "response", par.vals = list( minsplit = 1, minbucket = 1, cp = 0))
cv <- makeResampleDesc("CV", iters = 5)
r <- resample(learner, task, cv, measures=list(acc))
ACC_cros<- r$aggr
kable( data.frame(kroswalidacja=ACC_cros, podstawinie=ACC_org))
}
ACC<-ACC_org_coss(wina, tree)
ACC
tree_table<-data.frame(printcp(tree))
min_xerror<-min(tree_table$xerror)
index<-which.min(tree_table$xerror)
xstd<-tree_table[which.min(tree_table$xerror),5]
best_param<-which.max(tree_table[1:(index-1),4] > min_xerror & tree_table[1:(index-1),4] < (xstd+min_xerror))
tree_param <- rpart(Klasa ~ ., wina, minsplit = 1 , minbucket = 1, cp = tree_table[best_param,1])
rpart.plot(tree_param, type = 1, extra = 1)
ACC_param<-ACC_org_coss(wina, tree_param)
choose.cp <- function(tree) {
n <- which.min(tree$cptable[, 4])
n.min <- min(which(tree$cptable[, 4] < tree$cptable[n, 4] + tree$cptable[n, 5]))
return(tree$cptable[n.min, 1])
}
tree
trees<-lapply(2:14, function(i) rpart(Klasa~., wina[,1:i], minsplit = 1, minbucket = 1, cp = 0 ))
trees_param<- lapply(2:14, function(i) rpart(rpart(Klasa~.,  wina[,1:i], minsplit = 1, minbucket = 1, cp=choose.cp(trees[i-1]))))
trees_param<- lapply(2:14, function(i) rpart(rpart(Klasa~.,  wina[1:i], minsplit = 1, minbucket = 1, cp=choose.cp(trees[i-1]))))
treess<-lapply(2:14, function(i) rpart(Klasa~., wina[,1:i], minsplit = 1, minbucket = 1, cp = choose.cp(treess[(i-1)]) ))
trees<-lapply(2:14, function(i) rpart(Klasa~., wina[,1:i], minsplit = 1, minbucket = 1, cp = 0 ))
treess<-lapply(2:14, function(i) rpart(Klasa~., wina[,1:i], minsplit = 1, minbucket = 1, cp = choose.cp(treess[(i-1)]) ))
treess<-lapply(2:14, function(i) rpart(Klasa~., wina[,1:i], minsplit = 1, minbucket = 1, cp = choose.cp(trees[(i-1)]) ))
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(rpart)
library(rpart.plot)
library(mlr)
library (knitr)
library(rmarkdown)
set.seed(198)
rm(list=ls())
wina<-read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data", sep =",");
colnames(wina)<-c("Klasa", "Alkohol", "Kwas_jablkowy", "Popiol", "Alkaicznosc_popiolu", "Magnez","Calkowite_fenole", "Flawonoidy", "Nonflawanoidowe_fenole", "Proantocyjany", "Intensywnosc_koloru", "Odcien",  "Rozcienczenie_win", "Prolina" )
tree <- rpart(Klasa ~ ., wina, minsplit = 1, minbucket = 1, cp = 0)
rpart.plot(tree, type = 1, extra = 1)
ACC_org_coss<-function(wina, tree){
ACC_org<-sum(diag(table(wina$Klasa, predict(tree, wina))))/sum(table(wina$Klasa, predict(tree, wina)))
task<-mlr::makeClassifTask(data=wina, target = "Klasa")
learner<-makeLearner("classif.rpart", predict.type = "response", par.vals = list( minsplit = 1, minbucket = 1, cp = 0))
cv <- makeResampleDesc("CV", iters = 5)
r <- resample(learner, task, cv, measures=list(acc))
ACC_cros<- r$aggr
kable( data.frame(kroswalidacja=ACC_cros, podstawinie=ACC_org))
}
ACC<-ACC_org_coss(wina, tree)
ACC
tree_table<-data.frame(printcp(tree))
min_xerror<-min(tree_table$xerror)
index<-which.min(tree_table$xerror)
xstd<-tree_table[which.min(tree_table$xerror),5]
best_param<-which.max(tree_table[1:(index-1),4] > min_xerror & tree_table[1:(index-1),4] < (xstd+min_xerror))
tree_param <- rpart(Klasa ~ ., wina, minsplit = 1 , minbucket = 1, cp = tree_table[best_param,1])
rpart.plot(tree_param, type = 1, extra = 1)
ACC_param<-ACC_org_coss(wina, tree_param)
choose.cp <- function(tree) {
n <- which.min(tree$cptable[, 4])
n.min <- min(which(tree$cptable[, 4] < tree$cptable[n, 4] + tree$cptable[n, 5]))
return(tree$cptable[n.min, 1])
}
tree
trees<-lapply(2:14, function(i) rpart(Klasa~., wina[,1:i], minsplit = 1, minbucket = 1, cp = 0 ))
treess<-lapply(2:14, function(i) rpart(Klasa~., wina[,1:i], minsplit = 1, minbucket = 1, cp = choose.cp(trees[(i-1)])))
class(trees[1])
prune(trees[1], cp=choose.cp(trees[1]))
source('~/Desktop/Semestr_VI/LSED/lab6/przygotwanie_zad5.R', echo=TRUE)
# Tworzenie pełnego drzewa
tree0 <- rpart(class ~ ., data, minsplit = 0, minbucket = 0, cp = 0)
# Przycinanie drzewa
tree <- prune(tree0, cp = choose.cp(tree0))
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(rpart)
library(rpart.plot)
library(mlr)
library (knitr)
library(rmarkdown)
set.seed(198)
rm(list=ls())
wina<-read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data", sep =",");
colnames(wina)<-c("Klasa", "Alkohol", "Kwas_jablkowy", "Popiol", "Alkaicznosc_popiolu", "Magnez","Calkowite_fenole", "Flawonoidy", "Nonflawanoidowe_fenole", "Proantocyjany", "Intensywnosc_koloru", "Odcien",  "Rozcienczenie_win", "Prolina" )
tree <- rpart(Klasa ~ ., wina, minsplit = 1, minbucket = 1, cp = 0)
rpart.plot(tree, type = 1, extra = 1)
ACC_org_coss<-function(wina, tree){
ACC_org<-sum(diag(table(wina$Klasa, predict(tree, wina))))/sum(table(wina$Klasa, predict(tree, wina)))
task<-mlr::makeClassifTask(data=wina, target = "Klasa")
learner<-makeLearner("classif.rpart", predict.type = "response", par.vals = list( minsplit = 1, minbucket = 1, cp = 0))
cv <- makeResampleDesc("CV", iters = 5)
r <- resample(learner, task, cv, measures=list(acc))
ACC_cros<- r$aggr
kable( data.frame(kroswalidacja=ACC_cros, podstawinie=ACC_org))
}
ACC<-ACC_org_coss(wina, tree)
ACC
tree_table<-data.frame(printcp(tree))
min_xerror<-min(tree_table$xerror)
index<-which.min(tree_table$xerror)
xstd<-tree_table[which.min(tree_table$xerror),5]
best_param<-which.max(tree_table[1:(index-1),4] > min_xerror & tree_table[1:(index-1),4] < (xstd+min_xerror))
tree_param <- rpart(Klasa ~ ., wina, minsplit = 1 , minbucket = 1, cp = tree_table[best_param,1])
rpart.plot(tree_param, type = 1, extra = 1)
ACC_param<-ACC_org_coss(wina, tree_param)
choose.cp <- function(tree) {
n <- which.min(tree$cptable[, 4])
n.min <- min(which(tree$cptable[, 4] < tree$cptable[n, 4] + tree$cptable[n, 5]))
return(tree$cptable[n.min, 1])
}
trees<-lapply(2:14, function(i) rpart(Klasa~., wina[,1:i], minsplit = 1, minbucket = 1, cp = 0 ))
treess<-lapply(2:14, function(i) rpart(Klasa~., wina[,1:i], minsplit = 1, minbucket = 1, cp = choose.cp(trees[(i-1)])))
class(trees[1])
prune(trees[1], cp=choose.cp(trees[1]))
prune(tree, choose.cp(tree))
prune(tree, cp=choose.cp(tree))
class(trees[[1])
class(trees[[1]])
prune(trees[[1]], cp=choose.cp(trees[[1]]))
knitr::opts_chunk$set(echo = TRUE)
library (knitr)
library(rpart)
library(rpart.plot)
library(mlr)
library (knitr)
library(rmarkdown)
set.seed(198)
rm(list=ls())
wina<-read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data", sep =",");
colnames(wina)<-c("Klasa", "Alkohol", "Kwas_jablkowy", "Popiol", "Alkaicznosc_popiolu", "Magnez","Calkowite_fenole", "Flawonoidy", "Nonflawanoidowe_fenole", "Proantocyjany", "Intensywnosc_koloru", "Odcien",  "Rozcienczenie_win", "Prolina" )
library(MASS)
library(rpart)
library(rpart.plot)
library(mlr)
library (knitr)
library(rmarkdown)
set.seed(198)
rm(list=ls())
wina<-read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data", sep =",");
colnames(wina)<-c("Klasa", "Alkohol", "Kwas_jablkowy", "Popiol", "Alkaicznosc_popiolu", "Magnez","Calkowite_fenole", "Flawonoidy", "Nonflawanoidowe_fenole", "Proantocyjany", "Intensywnosc_koloru", "Odcien",  "Rozcienczenie_win", "Prolina" )
```{r}
library(MASS)
library(rpart)
library(rpart.plot)
library(mlr)
library (knitr)
library(rmarkdown)
set.seed(198)
rm(list=ls())
wina<-read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data", sep =",");
colnames(wina)<-c("Klasa", "Alkohol", "Kwas_jablkowy", "Popiol", "Alkaicznosc_popiolu", "Magnez","Calkowite_fenole", "Flawonoidy", "Nonflawanoidowe_fenole", "Proantocyjany", "Intensywnosc_koloru", "Odcien",  "Rozcienczenie_win", "Prolina" )
```
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(rpart)
library(rpart.plot)
library(mlr)
library (knitr)
library(rmarkdown)
set.seed(198)
rm(list=ls())
wina<-read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data", sep =",");
colnames(wina)<-c("Klasa", "Alkohol", "Kwas_jablkowy", "Popiol", "Alkaicznosc_popiolu", "Magnez","Calkowite_fenole", "Flawonoidy", "Nonflawanoidowe_fenole", "Proantocyjany", "Intensywnosc_koloru", "Odcien",  "Rozcienczenie_win", "Prolina" )
tree <- rpart(Klasa ~ ., wina, minsplit = 1, minbucket = 1, cp = 0)
rpart.plot(tree, type = 1, extra = 1)
ACC_org_coss<-function(wina, tree){
ACC_org<-sum(diag(table(wina$Klasa, predict(tree, wina))))/sum(table(wina$Klasa, predict(tree, wina)))
task<-mlr::makeClassifTask(data=wina, target = "Klasa")
learner<-makeLearner("classif.rpart", predict.type = "response", par.vals = list( minsplit = 1, minbucket = 1, cp = 0))
cv <- makeResampleDesc("CV", iters = 5)
r <- resample(learner, task, cv, measures=list(acc))
ACC_cros<- r$aggr
kable( data.frame(kroswalidacja=ACC_cros, podstawinie=ACC_org))
}
ACC<-ACC_org_coss(wina, tree)
ACC
tree_table<-data.frame(printcp(tree))
min_xerror<-min(tree_table$xerror)
index<-which.min(tree_table$xerror)
xstd<-tree_table[which.min(tree_table$xerror),5]
best_param<-which.max(tree_table[1:(index-1),4] > min_xerror & tree_table[1:(index-1),4] < (xstd+min_xerror))
tree_param <- rpart(Klasa ~ ., wina, minsplit = 1 , minbucket = 1, cp = tree_table[best_param,1])
rpart.plot(tree_param, type = 1, extra = 1)
ACC_param<-ACC_org_coss(wina, tree_param)
---
title: "zadanie 4"
author: "Joanna Gajewska"
date: "17 April 2019"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#Polecenie:
Należy wykorzystac zbiór danych dotyczący win (Zad 2) do przetestowania algorytmu kosztu-złożoności. W szczególności należy:
a)wczytać dane,
b)nazwać kolumny (korzystając z opisu),
c)stworzyć pełne drzewo (liście z elementami jednej klasy),
d)narysować pełne drzewo,
e)sprawdzić skuteczność pełnego drzewa przez powtórne podstawienie oraz    kroswalidację,
f)za pomocą tabeli cp wybrać drzewo optymalne, narysować je i porównac     wyniki jego skuteczności z pełnym drzewem,
g)stworzyć drzewo dla pierwszych: dwóch, trzech, czterach, itd. zmiennych - za każdym razem wyznaczyć drzewo optymalne,
h)wykreślić skuteczność drzewa w funkcji liczby użytych zmiennych, a także różnice rozmiaru drzewa pełnego i optymalnego.
```{r}
library(MASS)
library(rpart)
library(rpart.plot)
library(mlr)
library (knitr)
library(rmarkdown)
set.seed(198)
rm(list=ls())
wina<-read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data", sep =",");
colnames(wina)<-c("Klasa", "Alkohol", "Kwas_jablkowy", "Popiol", "Alkaicznosc_popiolu", "Magnez","Calkowite_fenole", "Flawonoidy", "Nonflawanoidowe_fenole", "Proantocyjany", "Intensywnosc_koloru", "Odcien",  "Rozcienczenie_win", "Prolina" )
```
c, d ) Tworzę i rysuję pełne drzewo
```{r}
tree <- rpart(Klasa ~ ., wina, minsplit = 1, minbucket = 1, cp = 0)
rpart.plot(tree, type = 1, extra = 1)
```
e) sprawdzam skueczność pełnego drzewa
```{r}
ACC_org_coss<-function(wina, tree){
ACC_org<-sum(diag(table(wina$Klasa, predict(tree, wina))))/sum(table(wina$Klasa, predict(tree, wina)))
task<-mlr::makeClassifTask(data=wina, target = "Klasa")
learner<-makeLearner("classif.rpart", predict.type = "response", par.vals = list( minsplit = 1, minbucket = 1, cp = 0))
cv <- makeResampleDesc("CV", iters = 5)
r <- resample(learner, task, cv, measures=list(acc))
ACC_cros<- r$aggr
kable( data.frame(kroswalidacja=ACC_cros, podstawinie=ACC_org))
}
ACC<-ACC_org_coss(wina, tree)
ACC
```
f) drzewo najbardziej optyalne
```{r}
tree_table<-data.frame(printcp(tree))
min_xerror<-min(tree_table$xerror)
index<-which.min(tree_table$xerror)
xstd<-tree_table[which.min(tree_table$xerror),5]
best_param<-which.max(tree_table[1:(index-1),4] > min_xerror & tree_table[1:(index-1),4] < (xstd+min_xerror))
tree_param <- rpart(Klasa ~ ., wina, minsplit = 1 , minbucket = 1, cp = tree_table[best_param,1])
rpart.plot(tree_param, type = 1, extra = 1)
ACC_param<-ACC_org_coss(wina, tree_param)
```
```{r}
choose.cp <- function(tree) {
n <- which.min(tree$cptable[, 4])
n.min <- min(which(tree$cptable[, 4] < tree$cptable[n, 4] + tree$cptable[n, 5]))
return(tree$cptable[n.min, 1])
}
trees<-lapply(2:14, function(i) rpart(Klasa~., wina[,1:i], minsplit = 1, minbucket = 1, cp = 0 ))
trees_parm<-lapply(1:13, function(i) prune(trees[[i]], cp = choose.cp(trees[[i]])))```
knitr::opts_chunk$set(echo = TRUE)
ACC_param<-ACC_org_coss(wina, tree_param)
choose.cp <- function(tree) {
n <- which.min(tree$cptable[, 4])
n.min <- min(which(tree$cptable[, 4] < tree$cptable[n, 4] + tree$cptable[n, 5]))
return(tree$cptable[n.min, 1])
}
trees<-lapply(2:14, function(i) rpart(Klasa~., wina[,1:i], minsplit = 1, minbucket = 1, cp = 0 ))
trees_parm<-lapply(1:13, function(i) prune(trees[[i]], cp = choose.cp(trees[[i]])))```
trees
prune(trees[[1]], cp = choose.cp(trees[[1]]))
prune(trees[[5]], cp = choose.cp(trees[[5]]))
prune(trees[[13]], cp = choose.cp(trees[[13]]))
prune(trees[[12]], cp = choose.cp(trees[[12]]))
prune(trees[[11]], cp = choose.cp(trees[[11]]))
prune(trees[[10]], cp = choose.cp(trees[[10]]))
trees_parm<-lapply(1:13, function(i) prune(trees[[i]], cp = choose.cp(trees[[i]])))
tree_param
trees_parm<-lapply(1:13, function(i) prune(trees[[i]], cp = choose.cp(trees[[i]])))
View(trees_parm)
tree_param <- prune(tree, cp=choose.cp(tree))
rpart.plot(tree_param, type = 1, extra = 1)
ACC_param<-ACC_org_coss(wina, tree_param)
ACC_param
ACC_all<-lapply(1:13, function(i) ACC_org_coss(wina[,1:(i+1)], tree_param[[i]]))
predict(wina[,1:(i+1)], tree_param[[i]])
predict(wina[,1:(3+1)], tree_param[[i]])
predict(wina[,1:(3+1)], tree_param[[3]])
predict(wina[,1:4], tree_param[[3]])
predict(wina, tree_param[[13]])
predict(wina, tree_param)
predict(wina, tree)
predict(tree_param[[13]], wina)
predict(tree, wina)
predict(trees_param[[13]], wina)
trees_parm<-lapply(1:13, function(i) prune(trees[[i]], cp = choose.cp(trees[[i]])))
predict(trees_param[[13]], wina)
predict(trees_parm[[13]], wina)
ACC_all<-lapply(1:13, function(i) ACC_org_coss( tree_param[[i]], wina[,1:(i+1)]))
ACC_all<-lapply(1:13, function(i) ACC_org_coss( trees_parm[[i]], wina[,1:(i+1)]))
trees_parm<-lapply(1:13, function(i) prune(trees[[i]], cp = choose.cp(trees[[i]])))
ACC_all<-lapply(1:13, function(i) ACC_org_coss( trees_parm[[i]], wina[,1:(i+1)]))
ACC_org_coss( trees_parm[[3]], wina[,1:(3+1)]))
ACC_org_coss( trees_parm[[3]], wina[,1:(3+1)])
ACC_org_coss( trees_parm[[3]], wina)
predict(trees_parm[[13]], wina)
ACC_all<-lapply(1:13, function(i) ACC_org_coss( tree_param[[i]], wina))
ACC_all<-lapply(1:13, function(i) ACC_org_coss( trees_parm[[i]], wina))
setwd("~/Desktop/Semestr_VI/Wstęp do uczenia maszynowego/projekt1")
library(DataExplorer)
heloc_dataset_v1<-read.csv(file = "heloc_dataset_v1.csv")
heloc_no9 <- heloc_dataset_v1[heloc_dataset_v1$MSinceMostRecentTradeOpen != -9, ]
heloc_9 <- heloc_dataset_v1[heloc_dataset_v1$MSinceMostRecentTradeOpen == -9, ]
setwd("~/Desktop/Semestr_VI/Wstęp do uczenia maszynowego/projekt1")
setwd("~/Desktop/Semestr_VI/Wstęp do uczenia maszynowego/projekt1")
setwd("~/Desktop/Semestr_VI/Wstęp do uczenia maszynowego/projekt1")
library(knitr)
![Caption for the picture.](braki1.png)
setwd("~/Desktop/Semestr_VI/Wstęp do uczenia maszynowego/projekt1")
setwd("~/Desktop/Semestr_VI/Wstęp do uczenia maszynowego/projekt1/WUM_projekt1")
source('~/Desktop/Semestr_VI/Wstęp do uczenia maszynowego/projekt1/faza2.r', echo=TRUE)
source('~/Desktop/Semestr_VI/Wstęp do uczenia maszynowego/projekt1/faza2.r', echo=TRUE)
source('~/Desktop/Semestr_VI/Wstęp do uczenia maszynowego/projekt1/faza2.r', echo=TRUE)
n<-ncol(data_set)-1;
train_set<-heloc_no9
cv <- makeResampleDesc("CV", iters = 5)
task <- makeClassifTask(data = train_set, target = "RiskPerformance")
model_all_rf <- makeLearner("classif.randomForest",
predict.type = "prob",
ntree = 100), task)
model_all_rf <- makeLearner("classif.randomForest",
predict.type = "prob",
ntree = 100)
auc_all_rf<-r <- resample(model_all_rf, task, cv,measures=list(acc))
AUC<-auc_all_rf;
name<-"rf"
number_of_cols<-n
AUC
AUC<-auc_all_rf$aggr
AUC
model_all_svm <- makeLearner("classif.svm",
predict.type = "prob")
auc_all_svm<-resample(model_all_svm, task, cv,measures=list(acc))
auc_all_svm<-resample(model_all_svm, task, cv,measures=list(acc))
AUC<-append(AUC, auc_all_svm$aggr)
name<-append(name, "svm")
number_of_cols<-append(n, n)
model_all_rpart <- makeLearner("classif.rpart",
predict.type = "prob")
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
>>>>>>> Stashed changes
